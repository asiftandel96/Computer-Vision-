{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    ">AlexNet was designed by Hinton, winner of the 2012 ImageNet competition, and his student Alex Krizhevsky. It was also after that year that more and deeper neural networks were proposed, such as the excellent vgg, GoogleLeNet. Its official data model has an accuracy rate of 57.1% and top 1-5 reaches 80.2%. This is already quite outstanding for traditional machine learning classification algorithms.\n",
    "\n",
    "\n",
    "<img src='https://miro.medium.com/max/1838/1*qyc21qM0oxWEuRaj-XJKcw.png'>\n",
    "\n",
    "\n",
    "<img src='https://neurohive.io/wp-content/uploads/2018/10/AlexNet-1.png'>\n",
    "\n",
    ">The following table below explains the network structure of AlexNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<thead>\n",
    "\t<tr>\n",
    "\t\t<th>Size / Operation</th>\n",
    "\t\t<th>Filter</th>\n",
    "\t\t<th>Depth</th>\n",
    "\t\t<th>Stride</th>\n",
    "\t\t<th>Padding</th>\n",
    "\t\t<th>Number of Parameters</th>\n",
    "\t\t<th>Forward Computation</th>\n",
    "\t</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\t<tr>\n",
    "\t\t<td>3* 227 * 227</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv1 + Relu</td>\n",
    "\t\t<td>11 * 11</td>\n",
    "\t\t<td>96</td>\n",
    "\t\t<td>4</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>(11*11*3 + 1) * 96=34944</td>\n",
    "\t\t<td>(11*11*3 + 1) * 96 * 55 * 55=105705600</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>96 * 55 * 55</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>96 * 27 * 27</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Norm</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv2 + Relu</td>\n",
    "\t\t<td>5 * 5</td>\n",
    "\t\t<td>256</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td>(5 * 5 * 96 + 1) * 256=614656</td>\n",
    "\t\t<td>(5 * 5 * 96 + 1) * 256 * 27 * 27=448084224</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 27 * 27</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Norm</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv3 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>384</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 256 + 1) * 384=885120</td>\n",
    "\t\t<td>(3 * 3 * 256 + 1) * 384 * 13 * 13=149585280</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>384 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv4 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>384</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 384=1327488</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 384 * 13 * 13=224345472</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>384 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv5 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>256</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 256=884992</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 256 * 13 * 13=149563648</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 6 * 6</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropout (rate 0.5)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC6 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>256 * 6 * 6 * 4096=37748736</td>\n",
    "\t\t<td>256 * 6 * 6 * 4096=37748736</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>4096</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropout (rate 0.5)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC7 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>4096 * 4096=16777216</td>\n",
    "\t\t<td>4096 * 4096=16777216</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>4096</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC8 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>4096 * 1000=4096000</td>\n",
    "\t\t<td>4096 * 1000=4096000</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>1000 classes</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Overall</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>62369152=62.3 million</td>\n",
    "\t\t<td>1135906176=1.1 billion</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv VS FC</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>Conv:3.7million (6%) , FC: 58.6 million  (94% )</td>\n",
    "\t\t<td>Conv: 1.08 billion (95%) , FC: 58.6 million (5%)</td>\n",
    "\t</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does AlexNet achieve better results?\n",
    "\n",
    "1. **Relu activation function is used.**\n",
    "\n",
    "Relu function: f (x) = max (0, x)\n",
    "\n",
    "![alex1](img/alex512.png)\n",
    "\n",
    "ReLU-based deep convolutional networks are trained several times faster than tanh and sigmoid- based networks. The following figure shows the number of iterations for a four-layer convolutional network based on CIFAR-10 that reached 25% training error in tanh and ReLU:\n",
    "\n",
    "![alex1](img/alex612.png)\n",
    "\n",
    "2. **Standardization ( Local Response Normalization )**\n",
    "\n",
    "After using ReLU f (x) = max (0, x), you will find that the value after the activation function has no range like the tanh and sigmoid functions, so a normalization will usually be done after ReLU, and the LRU is a steady proposal (Not sure here, it should be proposed?) One method in neuroscience is called \"Lateral inhibition\", which talks about the effect of active neurons on its surrounding neurons.\n",
    "\n",
    "![alex1](img/alex3.jpg)\n",
    "\n",
    "\n",
    "3. **Dropout**\n",
    "\n",
    "Dropout is also a concept often said, which can effectively prevent overfitting of neural networks. Compared to the general linear model, a regular method is used to prevent the model from overfitting. In the neural network, Dropout is implemented by modifying the structure of the neural network itself. For a certain layer of neurons, randomly delete some neurons with a defined probability, while keeping the individuals of the input layer and output layer neurons unchanged, and then update the parameters according to the learning method of the neural network. In the next iteration, rerandom Remove some neurons until the end of training.\n",
    "\n",
    "\n",
    "![alex1](img/alex4.jpg)\n",
    "\n",
    "\n",
    "4. **Enhanced Data ( Data Augmentation )**\n",
    "\n",
    "\n",
    "\n",
    "**In deep learning, when the amount of data is not large enough, there are generally 4 solutions:**\n",
    "\n",
    ">  Data augmentation- artificially increase the size of the training set-create a batch of \"new\" data from existing data by means of translation, flipping, noise\n",
    "\n",
    ">  Regularization——The relatively small amount of data will cause the model to overfit, making the training error small and the test error particularly large. By adding a regular term after the Loss Function , the overfitting can be suppressed. The disadvantage is that a need is introduced Manually adjusted hyper-parameter.\n",
    "\n",
    ">  Dropout- also a regularization method. But different from the above, it is achieved by randomly setting the output of some neurons to zero\n",
    "\n",
    ">  Unsupervised Pre-training- use Auto-Encoder or RBM's convolution form to do unsupervised pre-training layer by layer, and finally add a classification layer to do supervised Fine-Tuning\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from tflearn) (8.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from tflearn) (1.19.5)\n",
      "Requirement already satisfied: six in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from tflearn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tflearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
    " Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "# (2) Get Data\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "x, y = oxflower17.load_data(one_hot=True)\n",
    "\n",
    "# (3) Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),\\\n",
    " strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling \n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(2068, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(2068))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# (4) Compile \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=model_selection.train_test_split(x,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 870 samples, validate on 218 samples\n",
      "Epoch 1/35\n",
      "870/870 [==============================] - 155s 178ms/step - loss: 3.1040 - accuracy: 0.1575 - val_loss: 51.5232 - val_accuracy: 0.1193\n",
      "Epoch 2/35\n",
      "870/870 [==============================] - 135s 155ms/step - loss: 2.7282 - accuracy: 0.2103 - val_loss: 28.7088 - val_accuracy: 0.1239\n",
      "Epoch 3/35\n",
      "870/870 [==============================] - 105s 121ms/step - loss: 2.4937 - accuracy: 0.2460 - val_loss: 5.3704 - val_accuracy: 0.1972\n",
      "Epoch 4/35\n",
      "870/870 [==============================] - 105s 121ms/step - loss: 2.1893 - accuracy: 0.3103 - val_loss: 1.8380 - val_accuracy: 0.3394\n",
      "Epoch 5/35\n",
      "870/870 [==============================] - 104s 119ms/step - loss: 2.0418 - accuracy: 0.3471 - val_loss: 2.2700 - val_accuracy: 0.3532\n",
      "Epoch 6/35\n",
      "870/870 [==============================] - 103s 119ms/step - loss: 1.8102 - accuracy: 0.3839 - val_loss: 2.6083 - val_accuracy: 0.3303\n",
      "Epoch 7/35\n",
      "870/870 [==============================] - 104s 120ms/step - loss: 1.6483 - accuracy: 0.4517 - val_loss: 2.1722 - val_accuracy: 0.3578\n",
      "Epoch 8/35\n",
      "870/870 [==============================] - 131s 151ms/step - loss: 1.6783 - accuracy: 0.4425 - val_loss: 2.3141 - val_accuracy: 0.3761\n",
      "Epoch 9/35\n",
      "870/870 [==============================] - 138s 159ms/step - loss: 1.7098 - accuracy: 0.4402 - val_loss: 4.4621 - val_accuracy: 0.1881\n",
      "Epoch 10/35\n",
      "870/870 [==============================] - 123s 141ms/step - loss: 1.5524 - accuracy: 0.4782 - val_loss: 2.6803 - val_accuracy: 0.3761\n",
      "Epoch 11/35\n",
      "870/870 [==============================] - 158s 181ms/step - loss: 1.6300 - accuracy: 0.4575 - val_loss: 2.0366 - val_accuracy: 0.4220\n",
      "Epoch 12/35\n",
      "870/870 [==============================] - 149s 171ms/step - loss: 1.3959 - accuracy: 0.5241 - val_loss: 1.9495 - val_accuracy: 0.4633\n",
      "Epoch 13/35\n",
      "870/870 [==============================] - 144s 165ms/step - loss: 1.3817 - accuracy: 0.5276 - val_loss: 2.4209 - val_accuracy: 0.3761\n",
      "Epoch 14/35\n",
      "870/870 [==============================] - 117s 135ms/step - loss: 1.3247 - accuracy: 0.5460 - val_loss: 3.2422 - val_accuracy: 0.3257\n",
      "Epoch 15/35\n",
      "870/870 [==============================] - 156s 179ms/step - loss: 1.3368 - accuracy: 0.5276 - val_loss: 2.8583 - val_accuracy: 0.4220\n",
      "Epoch 16/35\n",
      "870/870 [==============================] - 155s 179ms/step - loss: 1.2692 - accuracy: 0.5759 - val_loss: 3.5555 - val_accuracy: 0.3028\n",
      "Epoch 17/35\n",
      "870/870 [==============================] - 148s 170ms/step - loss: 1.2137 - accuracy: 0.5977 - val_loss: 2.5814 - val_accuracy: 0.4358\n",
      "Epoch 18/35\n",
      "870/870 [==============================] - 127s 146ms/step - loss: 1.0293 - accuracy: 0.6483 - val_loss: 2.2051 - val_accuracy: 0.3899\n",
      "Epoch 19/35\n",
      "870/870 [==============================] - 113s 130ms/step - loss: 1.0032 - accuracy: 0.6609 - val_loss: 2.2419 - val_accuracy: 0.4266\n",
      "Epoch 20/35\n",
      "870/870 [==============================] - 130s 149ms/step - loss: 1.0154 - accuracy: 0.6529 - val_loss: 2.1503 - val_accuracy: 0.4908\n",
      "Epoch 21/35\n",
      "870/870 [==============================] - 132s 152ms/step - loss: 0.9203 - accuracy: 0.6805 - val_loss: 1.9885 - val_accuracy: 0.4725\n",
      "Epoch 22/35\n",
      "870/870 [==============================] - 128s 147ms/step - loss: 1.1214 - accuracy: 0.6276 - val_loss: 2.7348 - val_accuracy: 0.4358\n",
      "Epoch 23/35\n",
      "870/870 [==============================] - 146s 168ms/step - loss: 1.0002 - accuracy: 0.6414 - val_loss: 1.8000 - val_accuracy: 0.4908\n",
      "Epoch 24/35\n",
      "870/870 [==============================] - 132s 152ms/step - loss: 0.8561 - accuracy: 0.6989 - val_loss: 1.8749 - val_accuracy: 0.5092\n",
      "Epoch 25/35\n",
      "870/870 [==============================] - 143s 164ms/step - loss: 0.7993 - accuracy: 0.7218 - val_loss: 2.0333 - val_accuracy: 0.4954\n",
      "Epoch 26/35\n",
      "870/870 [==============================] - 146s 168ms/step - loss: 0.8233 - accuracy: 0.7184 - val_loss: 2.1143 - val_accuracy: 0.5596\n",
      "Epoch 27/35\n",
      "870/870 [==============================] - 145s 167ms/step - loss: 0.7735 - accuracy: 0.7356 - val_loss: 2.1620 - val_accuracy: 0.4817\n",
      "Epoch 28/35\n",
      "870/870 [==============================] - 131s 151ms/step - loss: 0.7742 - accuracy: 0.7598 - val_loss: 2.0438 - val_accuracy: 0.5275\n",
      "Epoch 29/35\n",
      "870/870 [==============================] - 132s 152ms/step - loss: 0.6432 - accuracy: 0.7874 - val_loss: 2.1669 - val_accuracy: 0.5321\n",
      "Epoch 30/35\n",
      "870/870 [==============================] - 151s 174ms/step - loss: 0.8199 - accuracy: 0.7333 - val_loss: 2.4150 - val_accuracy: 0.5138\n",
      "Epoch 31/35\n",
      "870/870 [==============================] - 146s 167ms/step - loss: 0.6720 - accuracy: 0.7816 - val_loss: 2.0245 - val_accuracy: 0.5275\n",
      "Epoch 32/35\n",
      "870/870 [==============================] - 146s 168ms/step - loss: 0.6539 - accuracy: 0.7862 - val_loss: 2.2143 - val_accuracy: 0.5459\n",
      "Epoch 33/35\n",
      "870/870 [==============================] - 157s 181ms/step - loss: 0.7956 - accuracy: 0.7448 - val_loss: 3.8328 - val_accuracy: 0.2615\n",
      "Epoch 34/35\n",
      "870/870 [==============================] - 122s 140ms/step - loss: 0.6993 - accuracy: 0.7759 - val_loss: 1.7400 - val_accuracy: 0.5229\n",
      "Epoch 35/35\n",
      "870/870 [==============================] - 126s 145ms/step - loss: 0.4962 - accuracy: 0.8471 - val_loss: 2.0958 - val_accuracy: 0.5138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x180a8bcf978>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=64, epochs=50, verbose=1,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 54, 54, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 17, 17, 256)       2973952   \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 4, 4, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 1, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2068)              531476    \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 2068)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2068)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 2068)              8272      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2068)              4278692   \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 2068)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2068)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 2068)              8272      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 500)               1034500   \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 17)                8517      \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 11,983,729\n",
      "Trainable params: 11,971,705\n",
      "Non-trainable params: 12,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 9s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.335966054131003, 0.48161765933036804]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.3.0\n",
      "  Downloading Keras-2.3.0-py2.py3-none-any.whl (377 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from keras==2.3.0) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from keras==2.3.0) (1.19.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from keras==2.3.0) (1.0.8)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-5.4.1-cp36-cp36m-win_amd64.whl (209 kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from keras==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from keras==2.3.0) (3.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from keras==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: cached-property in c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages (from h5py->keras==2.3.0) (1.5.2)\n",
      "Installing collected packages: pyyaml, keras\n",
      "Successfully installed keras-2.3.0 pyyaml-5.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.3.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
